{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWHmlzr2GaW1WxOAKFI6R4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkata55s/ai-agents/blob/main/clip2-summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXbZa8U2bMVZ"
      },
      "outputs": [],
      "source": [
        "# 1 - Import required libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 - Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "id": "BuEbDiZcbobp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_text(text):\n",
        "  # Step 2: Tokenize the input text\n",
        "  # Add a directive prompt \"summarize: Focus on key impacts and industries:\" to guide the model for better summarization.\n",
        "  # Tokenize the text, convert it to PyTorch tensors, and truncate it to a maximum length of 512 tokens if necessary.\n",
        "  inputs = tokenizer.encode(\n",
        "      \"summarize: Focus on key impacts and industries: \" + text,\n",
        "      return_tensors=\"pt\",\n",
        "      max_length=512,\n",
        "      truncation=True\n",
        "  )\n",
        "\n",
        "  # Step 3: Generate a summary\n",
        "  # Generate a summary from the model using specific parameters:\n",
        "  # max_length: Limit the summary to a maximum of 40 tokens to keep it concise.\n",
        "  # min_length: Ensure the summary is at least 10 tokens long.\n",
        "  # length_penalty: Penalize longer outputs to prioritize brevity.\n",
        "  # num_beams: Use beam search with 5 beams for higher-quality text generation.\n",
        "  # early_stopping: Stop generation early when an acceptable output is found.\n",
        "  outputs = model.generate(\n",
        "      inputs,\n",
        "      max_length=40,\n",
        "      min_length=10,\n",
        "      length_penalty=3.5,\n",
        "      num_beams=5,\n",
        "      early_stopping=True\n",
        "  )\n",
        "\n",
        "  # Step 4: Decode the model output\n",
        "  # Decode the generated tokens back into human-readable text and remove special tokens (e.g., <pad>, <eos>).\n",
        "  summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  # Step 5: Post-process the summary\n",
        "  # Deduplicate sentences in the summary to improve clarity and readability.\n",
        "  unique_sentences = list(dict.fromkeys(summary.split(\". \")))\n",
        "  return \". \".join(unique_sentences)"
      ],
      "metadata": {
        "id": "ntmbfqUlb6hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Step 6: Define the input text\n",
        "  # Provide a sample text to summarize.\n",
        "  sample_text = (\n",
        "      \"Artificial intelligence is a rapidly growing field that involves the creation of \"\n",
        "      \"intelligent machines capable of performing tasks that typically require human intelligence. \"\n",
        "      \"It is being used in various industries, including healthcare, finance, and transportation, \"\n",
        "      \"to improve efficiency and solve complex problems.\"\n",
        "  )\n",
        "\n",
        "  # Step 7: Generate and print the summary\n",
        "  # Call the summarize_text function with the sample text and print the original text and its summary.\n",
        "  print(\"Original Text:\")\n",
        "  print(sample_text)\n",
        "  print(\"\\nSummary:\")\n",
        "  print(summarize_text(sample_text))"
      ],
      "metadata": {
        "id": "9_iUeV1NecgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "############explain the below code lines. # 2 - Initialize tokenizer and model tokenizer = AutoTokenizer.from_pretrained(\"t5-base\") model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n",
        "Sure! These lines initialize the tokenizer and the model:\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\"): This line loads a pre-trained tokenizer for the \"t5-base\" model. The tokenizer is responsible for converting raw text into a format that the model can understand (e.g., breaking text into tokens and converting them into numerical IDs).\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\"): This line loads the pre-trained \"t5-base\" model. AutoModelForSeq2SeqLM indicates that it's a sequence-to-sequence language model, which is suitable for tasks like summarization, translation, etc.\n",
        "############\n",
        "why \"t5-base\" has provided as parameter for the below code line. what if i dont provide parameter. tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "The parameter \"t5-base\" specifies the particular pre-trained model whose tokenizer you want to load. Different models have different tokenization schemes, vocabulary, and special tokens, so it's crucial to use the tokenizer that corresponds to the model you intend to use.\n",
        "\n",
        "If you don't provide a parameter (e.g., AutoTokenizer.from_pretrained()), the function would raise an error because it wouldn't know which tokenizer to load. The from_pretrained method requires a string identifier to locate and download the correct tokenizer configuration and vocabulary files.\n",
        "\n",
        "############\n",
        "show me the list of all possible pre-trained models and their respective tokenizers, in table format.\n",
        "A comprehensive list of all possible pre-trained models and their respective tokenizers is extensive and constantly updated on the Hugging Face Model Hub. It's not practical to list them all in a static table here.\n",
        "\n",
        "However, you can easily explore all available models and their tokenizers directly on the Hugging Face Model Hub:\n",
        "\n",
        "Go to [redacted link]\n",
        "You can use the filters on the left sidebar to narrow down your search, for example, by task (like 'summarization' or 'text-generation'), library ('Transformers'), or language.\n",
        "Each model page will specify its identifier (e.g., \"t5-base\", \"bert-base-uncased\") which you can then use with AutoTokenizer.from_pretrained() and AutoModelForSeq2SeqLM.from_pretrained() (or AutoModel.from_pretrained() depending on the model architecture).\n",
        "This method ensures you always get the most current and complete list of available models and their documentation.\n",
        "\n",
        "############\n",
        "explain method summarize_text\n",
        "Certainly! The summarize_text method is designed to take an input text and generate a concise summary using the pre-trained T5 model. Here's a breakdown of what each step within the function does:\n",
        "\n",
        "Step 2: Tokenize the input text\n",
        "\n",
        "inputs = tokenizer.encode(...): This line takes the input text, prepends it with the directive prompt \"summarize: Focus on key impacts and industries: \" to guide the model, and then uses the tokenizer to convert this combined text into numerical input IDs. It also specifies return_tensors=\"pt\" to get PyTorch tensors and max_length=512, truncation=True to ensure the input doesn't exceed the model's maximum sequence length.\n",
        "Step 3: Generate a summary\n",
        "\n",
        "outputs = model.generate(...): This is where the actual summary generation happens. The model takes the tokenized inputs and generates a summary based on several parameters:\n",
        "max_length=40: The summary will have a maximum of 40 tokens.\n",
        "min_length=10: The summary will have a minimum of 10 tokens.\n",
        "length_penalty=3.5: This penalizes longer summaries, encouraging brevity.\n",
        "num_beams=5: Uses beam search with 5 beams, which helps in finding a higher-quality summary by exploring multiple possibilities.\n",
        "early_stopping=True: The generation stops early if a satisfactory output is found.\n",
        "Step 4: Decode the model output\n",
        "\n",
        "summary = tokenizer.decode(outputs[0], skip_special_tokens=True): After the model generates numerical outputs, this line uses the tokenizer to convert these numerical IDs back into human-readable text. outputs[0] refers to the first (and in this case, only) generated summary, and skip_special_tokens=True removes special tokens like \"<pad>\" or \"<eos>\" that are internal to the model.\n",
        "Step 5: Post-process the summary\n",
        "\n",
        "unique_sentences = list(dict.fromkeys(summary.split(\". \"))):\n",
        "return \". \".join(unique_sentences): This step aims to improve the summary's readability by removing any duplicate sentences. It first splits the summary into sentences, uses a dict.fromkeys() trick to get unique sentences while preserving order, and then joins them back into a single string."
      ],
      "metadata": {
        "id": "NOiO5Upt4YQ4"
      }
    }
  ]
}